{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kkhy9bFEsb2"
      },
      "outputs": [],
      "source": [
        "# Tomado de: https://www.analyticsvidhya.com/blog/2020/10/multivariate-multi-step-time-series-forecasting-using-stacked-lstm-sequence-to-sequence-autoencoder-in-tensorflow-2-0-keras/\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns \n",
        "import networkx as nx\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOyQp1ntOodI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSXWjOmleW_e"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/gdrive/Shareddrives/Network Science/PROYECTO/Serie de tiempo/serie.csv', sep=',')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D_1F__t3X84"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhaKurZh47BE"
      },
      "outputs": [],
      "source": [
        "data['Distrito'] = pd.to_numeric(data['Distrito'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXxcy9g-4MEa"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv2XCUjB3nXE"
      },
      "outputs": [],
      "source": [
        "# Conversion de los datos a numpy array\n",
        "valores = data.values\n",
        "\n",
        "# Construcion de escalador\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler = scaler.fit(valores)\n",
        "print(scaler)\n",
        "\n",
        "# Escalamiento de los valores\n",
        "normalizados = scaler.transform(valores)\n",
        "df_normalizados = pd.DataFrame(normalizados,\n",
        "                              index=data.index,\n",
        "                              columns=data.columns)\n",
        "df_normalizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKKkZ4NKOr_n"
      },
      "outputs": [],
      "source": [
        "data.info()\n",
        "y = np.array(df_normalizados['Distrito'])\n",
        "X = np.array(df_normalizados[['Source','Target','Time']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-CSvKKzFA69"
      },
      "outputs": [],
      "source": [
        "df_normalizados.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLymZYZcHxhZ"
      },
      "outputs": [],
      "source": [
        "G = nx.from_pandas_edgelist(data, source='Source', target='Target', edge_attr=[\"Distrito\",\"Time\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpYv24OiKObQ"
      },
      "outputs": [],
      "source": [
        "nx.convert_node_labels_to_integers(G)\n",
        "degree_centrality = nx.degree_centrality(G)  \n",
        "sorted(degree_centrality.items(), key=lambda x:x[1], reverse = True)[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcJQCVntLDBx"
      },
      "outputs": [],
      "source": [
        "closeness_centrality = nx.closeness_centrality(G)  \n",
        "sorted(closeness_centrality.items(), key=lambda x:x[1], reverse = True)[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNXT4xbYLtIR"
      },
      "outputs": [],
      "source": [
        "betweenness_centrality = nx.betweenness_centrality(G) \n",
        "sorted(betweenness_centrality.items(), key=lambda x:x[1], reverse = True)[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjXXY4yBX6jl"
      },
      "outputs": [],
      "source": [
        "daily_df = df_normalizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbe5b1tqGAuc"
      },
      "outputs": [],
      "source": [
        "train_number = int (len(df_normalizados)*0.7)\n",
        "train_df,test_df = daily_df[0:train_number], daily_df[train_number+1:len(df_normalizados)] \n",
        "train_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwcR_NmCGV_L"
      },
      "outputs": [],
      "source": [
        "len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDHu4pP8GCd3"
      },
      "outputs": [],
      "source": [
        "train = train_df\n",
        "scalers={}\n",
        "for i in train_df.columns:\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+ i] = scaler\n",
        "    train[i]=s_s\n",
        "test = test_df\n",
        "for i in train_df.columns:\n",
        "    scaler = scalers['scaler_'+i]\n",
        "    s_s = scaler.transform(test[i].values.reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+i] = scaler\n",
        "    test[i]=s_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmmtGUTKG7-7"
      },
      "outputs": [],
      "source": [
        "def split_series(series, n_past, n_future):\n",
        "  #\n",
        "  # n_past ==> no of past observations\n",
        "  #\n",
        "  # n_future ==> no of future observations \n",
        "  #\n",
        "  X, y = list(), list()\n",
        "  for window_start in range(len(series)):\n",
        "    past_end = window_start + n_past\n",
        "    future_end = past_end + n_future\n",
        "    if future_end > len(series):\n",
        "      break\n",
        "    # slicing the past and future parts of the window\n",
        "    past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
        "    X.append(past)\n",
        "    y.append(future)\n",
        "  return np.array(X), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyg5U3CIHAu4"
      },
      "outputs": [],
      "source": [
        "n_past = 10\n",
        "n_future = 2\n",
        "n_features = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz1mdgcsHDeL"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = split_series(train.values,n_past, n_future)\n",
        "X_train = X_train.reshape((X_train.shape[0], n_past, -1))\n",
        "y_train = y_train.reshape((y_train.shape[0], n_future, -1))\n",
        "X_test, y_test = split_series(test.values,n_past, n_future)\n",
        "X_test = X_test.reshape((X_test.shape[0], n_past, -1))\n",
        "y_test = y_test.reshape((y_test.shape[0], n_future, -1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hxz_KPMHLim"
      },
      "outputs": [],
      "source": [
        "# E1D1\n",
        "# n_features ==> no of features at each timestep in the data.\n",
        "#\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
        "encoder_l1 = tf.keras.layers.LSTM(100, return_state=True)\n",
        "encoder_outputs1 = encoder_l1(encoder_inputs)\n",
        "\n",
        "encoder_states1 = encoder_outputs1[1:]\n",
        "\n",
        "#\n",
        "decoder_inputs = tf.keras.layers.RepeatVector(n_future)(encoder_outputs1[0])\n",
        "\n",
        "#\n",
        "decoder_l1 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
        "decoder_outputs1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l1)\n",
        "\n",
        "#\n",
        "model_e1d1 = tf.keras.models.Model(encoder_inputs,decoder_outputs1)\n",
        "\n",
        "#\n",
        "model_e1d1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxgOJk-EHSmY"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "log_dir = \"/content/gdrive/Shareddrives/Network Science/PROYECTO/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_e1d1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(), \n",
        "    loss='categorical_crossentropy',\n",
        "    weighted_metrics=[\"acc\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_e1d1 = model_e1d1.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    validation_data=(X_test,y_test),\n",
        "    batch_size=32,\n",
        "    verbose=0,\n",
        "    callbacks=[tensorboard_callback]\n",
        "    # callbacks=[EarlyStopping(monitor='acc', patience=20)],\n",
        ")"
      ],
      "metadata": {
        "id": "IV_-DV8n_kxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcHo-qfrHkvF"
      },
      "outputs": [],
      "source": [
        "pred_e1d1=model_e1d1.predict(X_test)\n",
        "pred_e1d1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOnZTsUeKB4m"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9KQCdYkif-t"
      },
      "outputs": [],
      "source": [
        "pred_e1d1=model_e1d1.predict(X_test)\n",
        "\n",
        "    \n",
        "len(pred_e1d1)\n",
        "pred_e1d1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.Huber()\n",
        "\n",
        "model_e1d2.compile(\n",
        "    optimizer=optimizer, \n",
        "    loss=loss_fn,\n",
        "    weighted_metrics=[\"acc\"]\n",
        ")\n",
        "history_e1d1 = model_e1d2.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    validation_data=(X_test,y_test),\n",
        "    batch_size=32,\n",
        "    verbose=0,\n",
        "    callbacks=[EarlyStopping(monitor='acc', patience=20)],\n",
        ")"
      ],
      "metadata": {
        "id": "Hw0i8w7vcKvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31loKvD0rbSO"
      },
      "outputs": [],
      "source": [
        "def display_learning_curves(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(history.history[\"loss\"])\n",
        "    ax1.plot(history.history[\"val_loss\"])\n",
        "    ax1.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "    ax2.plot(history.history[\"acc\"])\n",
        "    ax2.plot(history.history[\"val_acc\"])\n",
        "    ax2.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax2.set_xlabel(\"Epochs\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_learning_curves(history_e1d1)"
      ],
      "metadata": {
        "id": "jlr9renpdBS1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}